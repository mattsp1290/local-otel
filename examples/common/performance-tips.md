# üöÄ Telemetry Performance Optimization Guide

This guide provides practical tips to minimize telemetry overhead while maintaining comprehensive observability. Use these patterns to ensure your telemetry doesn't become a performance bottleneck.

## üìä Performance Impact Overview

Typical telemetry overhead in well-optimized systems:
- **CPU**: 1-3% additional usage
- **Memory**: 10-50MB additional usage
- **Network**: <1% of total bandwidth
- **Latency**: <1ms per request

## üéØ General Optimization Principles

### 1. Measure Before Optimizing
```python
import time
import psutil
import gc

# Baseline without telemetry
gc.collect()
start_memory = psutil.Process().memory_info().rss / 1024 / 1024
start_time = time.time()

# Your operation here
process_without_telemetry()

baseline_time = time.time() - start_time
baseline_memory = psutil.Process().memory_info().rss / 1024 / 1024 - start_memory

# With telemetry
gc.collect()
start_memory = psutil.Process().memory_info().rss / 1024 / 1024
start_time = time.time()

with tracer.start_as_current_span("operation"):
    process_with_telemetry()

telemetry_time = time.time() - start_time
telemetry_memory = psutil.Process().memory_info().rss / 1024 / 1024 - start_memory

print(f"Time overhead: {((telemetry_time - baseline_time) / baseline_time) * 100:.2f}%")
print(f"Memory overhead: {telemetry_memory - baseline_memory:.2f}MB")
```

### 2. Start with Sampling
The most effective way to reduce overhead is to collect less data:
```python
from opentelemetry.sdk.trace.sampling import (
    TraceIdRatioBased,
    ParentBased,
    AlwaysOff,
    AlwaysOn
)

# Production sampling strategies
def create_sampler(environment):
    if environment == "production":
        # Sample 1% of traces in production
        return ParentBased(root=TraceIdRatioBased(0.01))
    elif environment == "staging":
        # Sample 10% in staging
        return ParentBased(root=TraceIdRatioBased(0.1))
    else:
        # Sample everything in development
        return AlwaysOn()
```

## üîç Tracing Optimizations

### 1. Selective Instrumentation
Don't trace everything - focus on valuable operations:

```python
# ‚ùå Bad - Too much instrumentation
def get_user_data(user_id):
    with tracer.start_as_current_span("get_user_data"):
        with tracer.start_as_current_span("validate_user_id"):
            if not user_id:
                return None
        
        with tracer.start_as_current_span("check_cache"):
            cached = cache.get(user_id)
        
        if cached:
            with tracer.start_as_current_span("parse_cached_data"):
                return json.loads(cached)
        
        with tracer.start_as_current_span("query_database"):
            return db.query(user_id)

# ‚úÖ Good - Strategic instrumentation
def get_user_data(user_id):
    with tracer.start_as_current_span("get_user_data") as span:
        span.set_attribute("user.id", user_id)
        span.set_attribute("cache.hit", False)
        
        cached = cache.get(user_id)
        if cached:
            span.set_attribute("cache.hit", True)
            return json.loads(cached)
        
        # Only trace expensive operations
        with tracer.start_as_current_span("database_query") as db_span:
            result = db.query(user_id)
            db_span.set_attribute("db.rows_returned", len(result))
            return result
```

### 2. Efficient Attribute Usage
Minimize attribute overhead:

```python
# ‚ùå Bad - Large attributes
span.set_attribute("request.body", json.dumps(large_payload))  # Could be MBs!
span.set_attribute("response.full", entire_response)

# ‚úÖ Good - Selective attributes
span.set_attribute("request.size_bytes", len(json.dumps(payload)))
span.set_attribute("request.content_type", "application/json")
span.set_attribute("response.status_code", 200)
span.set_attribute("response.size_bytes", len(response))
```

### 3. Span Batching Configuration
Optimize the span processor:

```python
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Optimized for high-throughput
high_throughput_processor = BatchSpanProcessor(
    span_exporter,
    max_queue_size=5000,  # Increased from default 2048
    max_export_batch_size=1000,  # Increased from default 512
    schedule_delay_millis=2000,  # Reduced from default 5000
    export_timeout_millis=10000  # Reduced from default 30000
)

# Optimized for low-latency
low_latency_processor = BatchSpanProcessor(
    span_exporter,
    max_queue_size=1000,  # Reduced buffer
    max_export_batch_size=100,  # Smaller batches
    schedule_delay_millis=500,  # More frequent exports
    export_timeout_millis=5000
)
```

### 4. Context Propagation Optimization
Minimize context size:

```python
from opentelemetry import baggage

# ‚ùå Bad - Large baggage items
baggage.set_baggage("user_profile", json.dumps(entire_user_object))

# ‚úÖ Good - Minimal baggage
baggage.set_baggage("user.id", str(user_id))
baggage.set_baggage("user.tier", "premium")  # Small, categorical values
```

## üìà Metrics Optimizations

### 1. Pre-aggregation
Aggregate metrics before sending:

```python
# ‚ùå Bad - High cardinality metrics
for user_id in user_ids:
    statsd_client.incr('api.calls', tags=[f'user_id:{user_id}'])

# ‚úÖ Good - Pre-aggregated metrics
from collections import defaultdict

class MetricsAggregator:
    def __init__(self, flush_interval=10):
        self.counters = defaultdict(int)
        self.last_flush = time.time()
        
    def increment(self, metric, tags=None):
        key = f"{metric}:{','.join(tags or [])}"
        self.counters[key] += 1
        
        if time.time() - self.last_flush > self.flush_interval:
            self.flush()
    
    def flush(self):
        for key, count in self.counters.items():
            metric, tags = key.split(':', 1)
            statsd_client.incr(metric, count, tags=tags.split(','))
        self.counters.clear()
        self.last_flush = time.time()
```

### 2. Histogram Optimization
Use histograms efficiently:

```python
# ‚ùå Bad - Too many histograms
for endpoint in endpoints:
    for status_code in range(200, 600):
        statsd_client.timing(
            f'http.request.duration',
            duration,
            tags=[f'endpoint:{endpoint}', f'status:{status_code}']
        )

# ‚úÖ Good - Grouped histograms
status_group = "2xx" if 200 <= status_code < 300 else \
               "3xx" if 300 <= status_code < 400 else \
               "4xx" if 400 <= status_code < 500 else "5xx"

statsd_client.timing(
    'http.request.duration',
    duration,
    tags=[
        f'endpoint:{endpoint}',
        f'status_group:{status_group}',
        f'method:{method}'
    ]
)
```

### 3. Metric Buffering
Buffer metrics in high-throughput scenarios:

```python
class BufferedStatsD:
    def __init__(self, client, buffer_size=100):
        self.client = client
        self.buffer = []
        self.buffer_size = buffer_size
        
    def incr(self, metric, value=1, tags=None):
        self.buffer.append(('incr', metric, value, tags))
        if len(self.buffer) >= self.buffer_size:
            self.flush()
    
    def flush(self):
        with self.client.pipeline() as pipe:
            for op, metric, value, tags in self.buffer:
                if op == 'incr':
                    pipe.incr(metric, value, tags=tags)
                elif op == 'gauge':
                    pipe.gauge(metric, value, tags=tags)
                elif op == 'timing':
                    pipe.timing(metric, value, tags=tags)
        self.buffer.clear()
```

## üìù Logging Optimizations

### 1. Log Level Management
Use appropriate log levels:

```python
import logging
import os

# Dynamic log level based on environment
def setup_logging():
    level = logging.INFO  # Default
    
    if os.getenv('DEBUG') == 'true':
        level = logging.DEBUG
    elif os.getenv('ENVIRONMENT') == 'production':
        level = logging.WARNING
    
    logging.basicConfig(level=level)
    
    # Reduce noise from libraries
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('botocore').setLevel(logging.WARNING)
```

### 2. Structured Logging Performance
Optimize JSON serialization:

```python
import orjson  # Faster JSON library

class FastJSONFormatter(logging.Formatter):
    def format(self, record):
        log_obj = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "message": record.getMessage(),
            "logger": record.name
        }
        
        # Only add trace context if available
        span = trace.get_current_span()
        if span and span.is_recording():
            ctx = span.get_span_context()
            if ctx.trace_id:
                log_obj["trace_id"] = format(ctx.trace_id, '032x')
                log_obj["span_id"] = format(ctx.span_id, '016x')
        
        # Use faster JSON serializer
        return orjson.dumps(log_obj).decode('utf-8')
```

### 3. Async Logging
Don't block on log writes:

```python
import asyncio
from queue import Queue
import threading

class AsyncLogger:
    def __init__(self, logger):
        self.logger = logger
        self.queue = Queue()
        self.worker = threading.Thread(target=self._worker)
        self.worker.daemon = True
        self.worker.start()
    
    def _worker(self):
        while True:
            level, msg, kwargs = self.queue.get()
            self.logger.log(level, msg, **kwargs)
    
    def info(self, msg, **kwargs):
        self.queue.put((logging.INFO, msg, kwargs))
```

## üè≠ Production Deployment Tips

### 1. Resource Limits
Set appropriate limits:

```yaml
# Kubernetes example
resources:
  limits:
    cpu: "100m"      # 0.1 CPU for telemetry overhead
    memory: "64Mi"   # 64MB for telemetry buffers
  requests:
    cpu: "50m"
    memory: "32Mi"
```

### 2. Circuit Breaker Pattern
Protect against telemetry system failures:

```python
class TelemetryCircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failures = 0
        self.threshold = failure_threshold
        self.timeout = timeout
        self.last_failure_time = 0
        self.is_open = False
    
    def call(self, func, *args, **kwargs):
        if self.is_open:
            if time.time() - self.last_failure_time > self.timeout:
                self.is_open = False  # Try to close
            else:
                return None  # Skip telemetry
        
        try:
            result = func(*args, **kwargs)
            self.failures = 0  # Reset on success
            return result
        except Exception as e:
            self.failures += 1
            self.last_failure_time = time.time()
            if self.failures >= self.threshold:
                self.is_open = True
                logger.warning(f"Telemetry circuit breaker opened: {e}")
            return None

# Usage
breaker = TelemetryCircuitBreaker()

def send_metrics(metric, value):
    breaker.call(statsd_client.incr, metric, value)
```

### 3. Gradual Rollout
Deploy telemetry incrementally:

```python
import random
import hashlib

def should_enable_telemetry(user_id, rollout_percentage):
    """Deterministic rollout based on user ID"""
    hash_value = int(hashlib.md5(str(user_id).encode()).hexdigest(), 16)
    return (hash_value % 100) < rollout_percentage

# Start with 1% rollout
if should_enable_telemetry(user_id, 1):
    tracer = trace.get_tracer(__name__)
else:
    tracer = NoOpTracer()  # Dummy tracer
```

## üìä Benchmarking Tools

### 1. Simple Performance Test
```python
import timeit

# Benchmark span creation
def benchmark_span_creation():
    with tracer.start_as_current_span("test") as span:
        span.set_attribute("test", "value")

time_per_span = timeit.timeit(benchmark_span_creation, number=10000) / 10000
print(f"Time per span: {time_per_span * 1000:.3f}ms")
```

### 2. Memory Profiling
```python
import tracemalloc

tracemalloc.start()

# Your telemetry code here
for i in range(1000):
    with tracer.start_as_current_span(f"span_{i}") as span:
        span.set_attribute("index", i)

current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.2f}MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.2f}MB")
tracemalloc.stop()
```

## üéØ Performance Targets

Recommended targets for production systems:

| Metric | Target | Red Flag |
|--------|--------|----------|
| Span creation time | <0.1ms | >1ms |
| Metric recording time | <0.01ms | >0.1ms |
| Log write time | <0.05ms | >0.5ms |
| Memory per 1000 spans | <10MB | >50MB |
| Network bandwidth | <1KB/s per instance | >10KB/s |
| CPU overhead | <2% | >5% |

## üîß Configuration Examples

### High-Performance Configuration
```python
# For high-throughput, latency-sensitive services
provider = TracerProvider(
    resource=resource,
    sampler=TraceIdRatioBased(0.001),  # 0.1% sampling
    span_processor=BatchSpanProcessor(
        exporter,
        max_queue_size=10000,
        max_export_batch_size=2000,
        schedule_delay_millis=5000
    )
)

# Minimal attributes
def get_minimal_span_attributes(request):
    return {
        "http.method": request.method,
        "http.route": request.route,
        "http.status_code": request.status_code
    }
```

### Balanced Configuration
```python
# For typical microservices
provider = TracerProvider(
    resource=resource,
    sampler=TraceIdRatioBased(0.01),  # 1% sampling
    span_processor=BatchSpanProcessor(
        exporter,
        max_queue_size=2048,
        max_export_batch_size=512,
        schedule_delay_millis=5000
    )
)
```

## üìö Key Takeaways

1. **Sampling is your friend** - It's better to have 1% of traces with full detail than 100% with missing data
2. **Measure impact** - Always benchmark before and after optimization
3. **Start conservative** - Begin with minimal telemetry and add more as needed
4. **Fail gracefully** - Telemetry should never break your application
5. **Optimize hotpaths** - Focus optimization efforts on high-frequency code paths

Remember: The goal is observability without observability becoming the problem!
